{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19bHZLB-5BPc7WYfQnjn1OQCm8DgXF_qd",
      "authorship_tag": "ABX9TyPq7FAhfuKQBuADepPpXXzt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo0pkGb43hdM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeimSZ2GNAYX"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCA1Y3Wn3mrO"
      },
      "source": [
        "PATHS = '/content/drive/MyDrive/DataAnalysis/전력사용량 예측 AI 경진대회/Data/'\n",
        "\n",
        "train = pd.read_csv(PATHS + 'train.csv', encoding='cp949')\n",
        "\n",
        "test = pd.read_csv(PATHS + 'test.csv', encoding='cp949')\n",
        "test = test.rename(columns = {\n",
        "    '강수량(mm, 6시간)' : '강수량(mm)',\n",
        "    '일조(hr, 3시간)' : '일조(hr)'\n",
        "    })\n",
        "\n",
        "submission = pd.read_csv(PATHS + 'sample_submission.csv', encoding='cp949')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI9ABLS8NB4n"
      },
      "source": [
        "# 피쳐엔지니어링 및 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKs7NFxo8iR0"
      },
      "source": [
        "def CDH(xs):\n",
        "  ys = []\n",
        "  for i in range(len(xs)):\n",
        "    if i < 11:\n",
        "      ys.append(np.sum(xs[:(i+1)]-26))\n",
        "    else:\n",
        "      ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
        "  return np.array(ys)\n",
        "\n",
        "def get_month_hour_mean(x, df):\n",
        "  month = x['month']\n",
        "  hour = x['hour']\n",
        "  ret = df[(month, hour)]\n",
        "  return ret\n",
        "\n",
        "def build_dataset(time_series, hour_series, seq_length):\n",
        "  dataX = []\n",
        "  dataY = []\n",
        "  dataHour = []\n",
        "\n",
        "  for i in range(0, len(time_series) - seq_length):\n",
        "    _x = time_series[i:i + seq_length + 1, :-1]\n",
        "    _y = time_series[i + seq_length, -1]  # Next close price\n",
        "    _hour = hour_series[i + seq_length]  # Next close price\n",
        "\n",
        "    dataX.append(_x)\n",
        "    dataY.append(_y)\n",
        "    dataHour.append(_hour)\n",
        "    \n",
        "  return np.array(dataX), np.array(dataY), np.array(dataHour)\n",
        "\n",
        "def get_data(train_df, test_df, num):\n",
        "\n",
        "  _train_df = train_df[train_df['num'] == num].reset_index(drop=True)\n",
        "  _train_df['date_time'] = pd.to_datetime(_train_df['date_time'])\n",
        "\n",
        "  _test_df = test_df[test_df['num'] == num].reset_index(drop=True)\n",
        "  _test_df['date_time'] = pd.to_datetime(test_df['date_time'])\n",
        "  _test_df = _test_df.interpolate(method='linear')\n",
        "\n",
        "  df = pd.concat([_train_df, _test_df]).reset_index(drop=True)\n",
        "\n",
        "  df['date_time'] = pd.to_datetime(df['date_time'])\n",
        "  \n",
        "  # 불쾌지수(DI)\n",
        "  # 9/5Ta-0.55(1-RH)(9/5Ta-26)+32\n",
        "  df['DI'] = 9/5*df['기온(°C)'] - 0.55*(1-df['습도(%)']/100)*(9/5*df['기온(°C)']-26)+32\n",
        "\n",
        "  # 시간\n",
        "  df['hour'] = df['date_time'].dt.hour\n",
        "\n",
        "  # 요일\n",
        "  df['weekday'] = df['date_time'].dt.weekday\n",
        "\n",
        "  # 달\n",
        "  df['month'] = df['date_time'].dt.month\n",
        "\n",
        "  # 냉방도일(CDH)\n",
        "  cdhs = np.array([])\n",
        "  cdh = CDH(df['기온(°C)'].values)\n",
        "  cdhs = np.concatenate([cdhs, cdh])\n",
        "  df['CDH'] = cdhs\n",
        "\n",
        "  # 시간 코사인\n",
        "  df['cos_time'] = np.cos(2*np.pi*(df['hour']/24))\n",
        "\n",
        "  # DP\n",
        "  c = 243.12\n",
        "  b = 17.62\n",
        "  gamma = (b * (df['기온(°C)']) / (c + (df['기온(°C)']))) + np.log(df['습도(%)'] / 100)\n",
        "  dp = ( c * gamma) / (b - gamma)\n",
        "  df['DP'] = dp\n",
        "\n",
        "  # 달, 시간 평균 온도\n",
        "  month_hour_temp = df.groupby(by = ['month', 'hour']).mean()['기온(°C)']\n",
        "  df['month_hour_temp'] = df.apply(lambda x : get_month_hour_mean(x, month_hour_temp), axis = 1)\n",
        "\n",
        "  # 달, 시간 평균 습도\n",
        "  month_hour_hu = df.groupby(by = ['month', 'hour']).mean()['습도(%)']\n",
        "  df['month_hour_hu'] = df.apply(lambda x : get_month_hour_mean(x, month_hour_hu), axis = 1)\n",
        "\n",
        "  # # 7일, 8일, 9일 전의 현재 시간의 전력 사용량의 평균 및 그 값\n",
        "  # df['shift1'] = df['전력사용량(kWh)'].shift(24 * 7)\n",
        "  # df['shift2'] = df['전력사용량(kWh)'].shift(24 * 8)\n",
        "  # df['shift3'] = df['전력사용량(kWh)'].shift(24 * 9)\n",
        "\n",
        "  # df['avg2'] = np.mean(df[['shift1', 'shift2']].values, axis=-1)\n",
        "  # df['avg3'] = np.mean(df[['shift1', 'shift2', 'shift3']].values, axis=-1)\n",
        "\n",
        "  # df = df[~df['avg3'].isna()]\n",
        "  # df = df.reset_index(drop=True)\n",
        "  _hour = df['month'].values\n",
        "\n",
        "  cols = ['기온(°C)', '풍속(m/s)', '습도(%)', '강수량(mm)', '일조(hr)',\n",
        "          'DI', 'hour', 'weekday', 'month', 'CDH', \n",
        "          'cos_time', 'DP', 'month_hour_temp', 'month_hour_hu',]\n",
        "\n",
        "  min = df[cols].values.min(axis=0)\n",
        "  max = df[cols].values.max(axis=0)\n",
        "\n",
        "  df.loc[:, cols] = (df[cols] - min) / (max - min)\n",
        "\n",
        "  cols = ['기온(°C)', '풍속(m/s)', '습도(%)', '강수량(mm)', '일조(hr)',\n",
        "          'DI', 'hour', 'weekday', 'month', 'CDH', \n",
        "          'cos_time', 'DP', 'month_hour_temp', 'month_hour_hu', '전력사용량(kWh)']\n",
        "\n",
        "  _X = df[cols].values\n",
        "\n",
        "  X, y, hour = build_dataset(time_series = _X, hour_series = _hour, seq_length = 71)\n",
        "\n",
        "  test_size = 24 * 7\n",
        "\n",
        "  X_train = X[ : -(test_size)]\n",
        "  y_train = y[ : -(test_size)]\n",
        "  Hour_train = hour[ : -(test_size)]\n",
        "\n",
        "  X_test = X[-test_size: ]\n",
        "  # y_test = y[-test_size: ]\n",
        "  # Hour_test = hour[-test_size: ]\n",
        "\n",
        "  return X_train, y_train, Hour_train, X_test\n",
        "\n",
        "  # return X_train, y_train, Hour_train, X_test, y_test, Hour_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYnJvJGjNENG"
      },
      "source": [
        "# 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDSBG2MU0NkO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Flatten,MaxPooling1D,BatchNormalization, Lambda, AveragePooling1D, Dropout, Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "def smape(true, pred):\n",
        "\n",
        "  v = 2 * abs(pred - true) / (abs(pred) + abs(true))\n",
        "  output = np.mean(v) * 100\n",
        "\n",
        "  return output\n",
        "\n",
        "def my_metric(true, pred):\n",
        "  score = tf.py_function(func=smape, inp=[true, pred], Tout=tf.float32,  name='custom_nmae')\n",
        "  return score\n",
        "\n",
        "def set_model():\n",
        "\n",
        "  nf = 16\n",
        "  fs = 3\n",
        "  padding = 'causal'\n",
        "  activation = 'elu'\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(keras.layers.InputLayer((72, 14)))\n",
        "\n",
        "  model.add(Conv1D(filters = nf, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv1D(filters = nf * 2, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv1D(filters = nf * 4, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv1D(filters = nf * 8, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv1D(filters = nf * 16, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv1D(filters = nf * 32, kernel_size = fs, padding = padding))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(activation = activation))\n",
        "  # model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation = activation))\n",
        "  model.add(Dense(128, activation = activation))\n",
        "  model.add(Dense(32, activation = activation))\n",
        "  model.add(Dense(8, activation = activation))\n",
        "  model.add(Dense(4, activation = activation)) \n",
        "  model.add(Dense(1))# output size \n",
        "\n",
        "  optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
        "\n",
        "  model.compile(loss = 'mae', optimizer = optimizer, metrics=[my_metric])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOlC2eWoNF-W"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjUamjcELdOn"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 100\n",
        "n_split = 5\n",
        "kfold = StratifiedKFold(n_splits = n_split, shuffle=True, random_state=22)\n",
        "PATHS = '/content/drive/MyDrive/DataAnalysis/전력사용량 예측 AI 경진대회/Model/'\n",
        "\n",
        "for num in tqdm_notebook(range(1, 61)):\n",
        "\n",
        "  X_train, y_train, Hour_train, X_test = get_data(train, test, num)\n",
        "  i = 0\n",
        "\n",
        "  train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=34)\n",
        "\n",
        "  model = set_model()\n",
        "  mc = ModelCheckpoint(PATHS + f'num_{num}_cv_study{i + 1}.h5', save_best_only=True, verbose=0, monitor = 'val_my_metric', mode = 'min', save_weights_only=True)\n",
        "  reLR = ReduceLROnPlateau(monitor = 'val_my_metric', patience = 7,verbose = 0,factor = 0.5)\n",
        "\n",
        "  history = model.fit(train_X, train_y, epochs = EPOCHS, validation_data = (val_X, val_y),\n",
        "        verbose = 0, batch_size=BATCH_SIZE, callbacks = [mc, reLR])\n",
        "\n",
        "  model.load_weights(PATHS + f'num_{num}_cv_study{i + 1}.h5')\n",
        "\n",
        "  k_accuracy = '%.4f' % (model.evaluate(val_X, val_y)[1])\n",
        "  k_loss = '%.4f' % (model.evaluate(val_X, val_y)[0])\n",
        "\n",
        "  print(f'num: {num}, Auc: {k_accuracy}, Loss: {k_loss}')\n",
        "\n",
        "  pred = model.predict(X_test)\n",
        "  submission.iloc[(24 * 7) * (num - 1) : (24 * 7) * (num), 1] = pred\n",
        "\n",
        "  # for i, (train_idx, val_idx) in enumerate(kfold.split(X_train, Hour_train)):\n",
        "  #   train_X, val_X = X_train[train_idx], X_train[val_idx]\n",
        "  #   train_y, val_y = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "  #   model = set_model()\n",
        "  #   mc = ModelCheckpoint(PATHS + f'num_{num}_cv_study{i + 1}.h5', save_best_only=True, verbose=0, monitor = 'val_my_metric', mode = 'min', save_weights_only=True)\n",
        "  #   reLR = ReduceLROnPlateau(monitor = 'val_my_metric', patience = 7,verbose = 0,factor = 0.5)\n",
        "\n",
        "  #   history = model.fit(train_X, train_y, epochs = EPOCHS, validation_data = (val_X, val_y),\n",
        "  #           verbose = 0, batch_size=BATCH_SIZE, callbacks = [mc, reLR])\n",
        "\n",
        "  #   model.load_weights(PATHS + f'num_{num}_cv_study{i + 1}.h5')\n",
        "\n",
        "  #   k_accuracy = '%.4f' % (model.evaluate(val_X, val_y)[1])\n",
        "  #   k_loss = '%.4f' % (model.evaluate(val_X, val_y)[0])\n",
        "\n",
        "  #   accuracy.append(k_accuracy)\n",
        "  #   losss.append(k_loss)\n",
        "\n",
        "  # print(f'num: {num}, Auc: {accuracy}, Loss: {losss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7F-d-pKh3bF"
      },
      "source": [
        "# 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZFdPCH4h5Ep"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 100\n",
        "n_split = 5\n",
        "kfold = StratifiedKFold(n_splits = n_split, shuffle=True, random_state=22)\n",
        "PATHS = '/content/drive/MyDrive/DataAnalysis/전력사용량 예측 AI 경진대회/Model/'\n",
        "\n",
        "for num in tqdm_notebook(range(1, 61)):\n",
        "  X_train, y_train, Hour_train, X_test = get_data(train, test, num)\n",
        "\n",
        "  accuracy = []\n",
        "  i_li = []\n",
        "\n",
        "  for i, (train_idx, val_idx) in enumerate(kfold.split(X_train, Hour_train)):\n",
        "    train_X, val_X = X_train[train_idx], X_train[val_idx]\n",
        "    train_y, val_y = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    model = set_model()\n",
        "    model.load_weights(PATHS + f'num_{num}_cv_study{i + 1}.h5')\n",
        "\n",
        "    k_accuracy = model.evaluate(val_X, val_y)[1]\n",
        "\n",
        "    accuracy.append(k_accuracy)\n",
        "\n",
        "    if k_accuracy <= 100:\n",
        "      i_li.append(i)\n",
        "\n",
        "  preds = []\n",
        "  for i in i_li:\n",
        "    model = set_model()\n",
        "    model.load_weights(PATHS + f'num_{num}_cv_study{i + 1}.h5')\n",
        "    pred = model.predict(X_test)\n",
        "    preds.append(pred)\n",
        "\n",
        "  print(f'num: {num}, 1_Auc: {accuracy[0]}, 2_Auc: {accuracy[1]}, 3_Auc: {accuracy[2]}, 4_Auc: {accuracy[3]}, 5_Auc: {accuracy[4]}, i_li: {i_li}, len_preds: {len(preds)} \\n')\n",
        "  \n",
        "  preds = np.mean(preds, axis=0)\n",
        "  submission.iloc[(24 * 7) * (num - 1) : (24 * 7) * (num), 1] = preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKL0ESpprF5g"
      },
      "source": [
        "PATHS = '/content/drive/MyDrive/DataAnalysis/전력사용량 예측 AI 경진대회/Predict/'\n",
        "submission.to_csv(PATHS + 'CNN1D_0616.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rqjGcxXsP6L"
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7xnREyru2Du"
      },
      "source": [
        "submission.tail()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}